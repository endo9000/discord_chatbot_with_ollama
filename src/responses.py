# responses.py
import ollama

def get_response(model: str, user_input: str) -> str:
    """
    Function to get response from a chatbot model based on user input.
    
    Args:
        model (str): The chatbot model to use for generating the response.
        user_input (str): The user input for the chatbot.
        
    Returns:
        str: The response generated by the chatbot model.
    """
    response = ollama.chat(
        model=model, 
        messages=[
            {
                'role': 'user',
                'content': user_input,
            }
        ],
        stream=True
    )
    return response


# DEBUG SECTION
def test_get_response():
    """
    A function that continuously takes user input, passes it to a model, and prints the response.
    """
    while True:
        user_input = input("YOU: ")
        stream = get_response(model="gemma:2b", user_input=user_input)
        for chunk in stream:
            print(chunk['message']['content'], end='', flush=True)
        print()
    
if __name__ == '__main__':
    test_get_response()