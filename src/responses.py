# responses.py
import ollama

def get_response(model: str, user_input: str) -> str:
    """
    Function to get response from a chatbot model based on user input.
    
    Args:
        model (str): The chatbot model to use for generating the response.
        user_input (str): The user input for the chatbot.
        
    Returns:
        str: The response generated by the chatbot model.
    """
    response = ollama.chat(
        model=model, 
        messages=[
            {
                'role': 'user',
                'content': user_input,
            }
        ],
        stream=True,
    )
    return response

# DEBUG SECTION
def test_get_response():
    """
    A function that continuously takes user input, passes it to a model, and prints the response.
    """
    while True:
        user_input = input("YOU: ")
        stream = get_response(model='gemma:2b', user_input=user_input)
        for chunk in stream:
            print(chunk['message']['content'], end='', flush=True)
        print()

if __name__ == '__main__':
    test_get_response()